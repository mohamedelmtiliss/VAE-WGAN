{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c3a21-dc0a-4f79-b507-f7afd123bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from models import VAE_WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820d09e-c68e-4456-bd7d-f6c5a531cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.0002\n",
    "EPOCHS = 50\n",
    "LATENT_DIM = 128\n",
    "# Point this to your folder containing the \"normal_reference\" subfolder\n",
    "DATA_PATH = 'modis_dataset/normal_reference' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc78e1d-18af-4267-9648-3ecd30c7443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Data\n",
    "# We use ImageFolder, so your data must be inside a subfolder \n",
    "# Structure: modis_dataset/normal_reference/images/img1.png\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe91afd-4d02-4291-91e6-13d88725a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy root folder structure if needed or point to parent\n",
    "# PyTorch ImageFolder expects: root/class_name/image.png\n",
    "# If your images are just in 'normal_reference/', point ImageFolder to 'modis_dataset/' \n",
    "# and ensure 'normal_reference' is the only folder inside relevant for training.\n",
    "dataset = datasets.ImageFolder(root='modis_dataset', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1100b5c-35c2-441c-a553-b851712294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: We only want the NORMAL class for training!\n",
    "# Assuming 'normal_reference' is class 0 or 1. \n",
    "# It is often easier to just point to a folder containing ONLY normal images.\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d111d5d-cda4-4c7d-a3f3-8b9fbb9a790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE_WGAN(latent_dim=LATENT_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea41f1-1fc2-4171-a0d3-70bdb6526d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_enc = optim.Adam(model.encoder.parameters(), lr=LR)\n",
    "optimizer_dec = optim.Adam(model.decoder.parameters(), lr=LR)\n",
    "optimizer_dis = optim.Adam(model.discriminator.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2739e-684d-481e-914d-c6032a823506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "criterion_recon = torch.nn.MSELoss()\n",
    "criterion_adv = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438366f9-d323-47f3-b18b-6f8dc1c9b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on {device}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799552bb-72f4-47d6-8a41-f5c653b2d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # --- A. Train Discriminator ---\n",
    "        # Real images = Label 1, Fake (Reconstructed) = Label 0\n",
    "        optimizer_dis.zero_grad()\n",
    "        \n",
    "        # 1. Real\n",
    "        real_labels = torch.ones(imgs.size(0)).to(device)\n",
    "        fake_labels = torch.zeros(imgs.size(0)).to(device)\n",
    "        \n",
    "        outputs_real = model.discriminator(imgs)\n",
    "        d_loss_real = criterion_adv(outputs_real, real_labels)\n",
    "        \n",
    "        # 2. Fake (Reconstruct images)\n",
    "        mu, logvar = model.encoder(imgs)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        recon_imgs = model.decoder(z)\n",
    "        \n",
    "        outputs_fake = model.discriminator(recon_imgs.detach()) # Detach to stop grad to generator\n",
    "        d_loss_fake = criterion_adv(outputs_fake, fake_labels)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_dis.step()\n",
    "\n",
    "        # --- B. Train Encoder & Decoder (Generator) ---\n",
    "        optimizer_enc.zero_grad()\n",
    "        optimizer_dec.zero_grad()\n",
    "        \n",
    "        # 1. Reconstruction Loss\n",
    "        # We want the output to look like the input\n",
    "        recon_loss = criterion_recon(recon_imgs, imgs)\n",
    "        \n",
    "        # 2. KL Divergence (Standard VAE Loss)\n",
    "        # Forces latent space to be normal distribution\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss /= (BATCH_SIZE * 64 * 64) # Normalize\n",
    "        \n",
    "        # 3. Adversarial Loss\n",
    "        # We want to fool the discriminator (make it predict 1 for fakes)\n",
    "        outputs_fake_for_gen = model.discriminator(recon_imgs)\n",
    "        adv_loss = criterion_adv(outputs_fake_for_gen, real_labels)\n",
    "        \n",
    "        # Total Generator Loss\n",
    "        # Weights (gamma) adjust importance. \n",
    "        # Typically Reconstruction is highest priority.\n",
    "        g_loss = (10 * recon_loss) + (0.1 * kl_loss) + adv_loss\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_enc.step()\n",
    "        optimizer_dec.step()\n",
    "        \n",
    "        total_loss += g_loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ae45f-8653-46b6-86ba-a5cf650faf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"vae_wgan_brazil_fire.pth\")\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
